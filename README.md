# CommentToxicityModelComment Toxicity Model using Long Short-Term Memory (LSTM)
This repository contains a comment toxicity model implemented using the Long Short-Term Memory (LSTM) algorithm. 
The model is designed to predict the toxicity level of text comments.

The model is built using the LSTM algorithm, a type of recurrent neural network (RNN) that excels at capturing 
long-term dependencies in sequential data. By training on a labeled dataset, the model learns patterns and contexts
associated with toxic comments.

To run the project, install dependencies and then run 
python toxicity.py

# Results
The model achieves promising results in predicting comment toxicity. However,
the accuracy and performance may vary depending on the dataset used for training and 
the complexity of the comment language.
To further improve the model's performance, consider experimenting with different 
hyperparameters, like you can increase the number of epoches and add dense layer.
